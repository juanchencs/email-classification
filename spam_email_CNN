{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import tensorflow.contrib.keras as kr\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "class TextConfig():\n",
    "    vocab_size = 5000\n",
    "    seq_length = 600\n",
    "    embedding_dim = 64  # 词向量维度\n",
    "    num_filters = 256  # 卷积核数目\n",
    "    kernel_size = 5  # 卷积核尺\n",
    "    hidden_dim = 128  # 全连接层神经元\n",
    "    dropout_keep_prob = 0.5  # dropout保留比例\n",
    "    learning_rate = 1e-3  # 学习率\n",
    "    batch_size = 32  # 每批训练大小\n",
    "    num_iteration = 5000 #迭代次数\n",
    "    print_per_batch = num_iteration / 20 #打印间隔\n",
    "\n",
    "class TextClassification():\n",
    "    def config(self):\n",
    "        textConfig = TextConfig()\n",
    "        self.vocab_size = textConfig.vocab_size\n",
    "        self.seq_length = textConfig.seq_length\n",
    "        self.embedding_dim = textConfig.embedding_dim\n",
    "        self.num_filters = textConfig.num_filters\n",
    "        self.kernel_size = textConfig.kernel_size\n",
    "        self.hidden_dim = textConfig.hidden_dim\n",
    "        self.dropout_keep_prob = textConfig.dropout_keep_prob\n",
    "        self.learning_rate = textConfig.learning_rate\n",
    "        self.batch_size = textConfig.batch_size\n",
    "        self.print_per_batch = textConfig.print_per_batch\n",
    "        self.num_iteration = textConfig.num_iteration\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        self.config()\n",
    "        if len(args) == 2:\n",
    "            content_list = args[0]\n",
    "            label_list = args[1]\n",
    "            train_X, test_X, train_y, test_y = train_test_split(content_list, label_list)\n",
    "            self.train_content_list = train_X\n",
    "            self.train_label_list = train_y\n",
    "            self.test_content_list = test_X\n",
    "            self.test_label_list = test_y\n",
    "            self.content_list = self.train_content_list + self.test_content_list\n",
    "        elif len(args) == 4:\n",
    "            self.train_content_list = args[0]\n",
    "            self.train_label_list = args[1]\n",
    "            self.test_content_list = args[2]\n",
    "            self.test_label_list = args[3]\n",
    "            self.content_list = self.train_content_list + self.test_content_list\n",
    "        else:\n",
    "            print('false to init TextClassification object')\n",
    "        self.autoGetNumClasses()\n",
    "    \n",
    "    def autoGetNumClasses(self):\n",
    "        label_list = self.train_label_list + self.test_label_list\n",
    "        self.num_classes = np.unique(label_list).shape[0]\n",
    "    \n",
    "    def getVocabularyList(self, content_list, vocabulary_size):\n",
    "        allContent_str = ''.join(content_list)\n",
    "        counter = Counter(allContent_str)\n",
    "        vocabulary_list = [k[0] for k in counter.most_common(vocabulary_size)]\n",
    "        return ['PAD'] + vocabulary_list\n",
    "\n",
    "    def prepareData(self):\n",
    "        vocabulary_list = self.getVocabularyList(self.content_list, self.vocab_size)\n",
    "        if len(vocabulary_list) < self.vocab_size:\n",
    "            self.vocab_size = len(vocabulary_list)\n",
    "        contentLength_list = [len(k) for k in self.train_content_list]\n",
    "        if max(contentLength_list) < self.seq_length:\n",
    "            self.seq_length = max(contentLength_list)\n",
    "        self.word2id_dict = dict([(b, a) for a, b in enumerate(vocabulary_list)])\n",
    "        self.labelEncoder = LabelEncoder()\n",
    "        self.labelEncoder.fit(self.train_label_list)\n",
    "\n",
    "    def content2idList(self, content):\n",
    "        return [self.word2id_dict[word] for word in content if word in self.word2id_dict]\n",
    "\n",
    "    def content2X(self, content_list):\n",
    "        idlist_list = [self.content2idList(content) for content in content_list]\n",
    "        X = kr.preprocessing.sequence.pad_sequences(idlist_list, self.seq_length)\n",
    "        return X\n",
    "\n",
    "    def label2Y(self, label_list):\n",
    "        y = self.labelEncoder.transform(label_list)\n",
    "        Y = kr.utils.to_categorical(y, self.num_classes)\n",
    "        return Y\n",
    "\n",
    "    def buildModel(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.X_holder = tf.placeholder(tf.int32, [None, self.seq_length])\n",
    "        self.Y_holder = tf.placeholder(tf.float32, [None, self.num_classes])\n",
    "        embedding = tf.get_variable('embedding', [self.vocab_size, self.embedding_dim])\n",
    "        embedding_inputs = tf.nn.embedding_lookup(embedding, self.X_holder)\n",
    "        conv = tf.layers.conv1d(embedding_inputs, self.num_filters, self.kernel_size)\n",
    "        max_pooling = tf.reduce_max(conv, reduction_indices=[1])\n",
    "        full_connect = tf.layers.dense(max_pooling, self.hidden_dim)\n",
    "        full_connect_dropout = tf.contrib.layers.dropout(full_connect, keep_prob=self.dropout_keep_prob)\n",
    "        full_connect_activate = tf.nn.relu(full_connect_dropout)\n",
    "        softmax_before = tf.layers.dense(full_connect_activate, self.num_classes)\n",
    "        self.predict_Y = tf.nn.softmax(softmax_before)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.Y_holder, logits=softmax_before)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train = optimizer.minimize(self.loss)\n",
    "        self.predict_y = tf.argmax(self.predict_Y, 1)\n",
    "        isCorrect = tf.equal(tf.argmax(self.Y_holder, 1), self.predict_y)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(isCorrect, tf.float32))\n",
    "\n",
    "    def trainModel(self):\n",
    "        self.prepareData()\n",
    "        self.buildModel()\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(init)\n",
    "        train_X = self.content2X(self.train_content_list)\n",
    "        train_Y = self.label2Y(self.train_label_list)\n",
    "        test_X = self.content2X(self.test_content_list)\n",
    "        test_Y = self.label2Y(self.test_label_list)\n",
    "        startTime = time.time()\n",
    "        for i in range(self.num_iteration):\n",
    "            selected_index = random.sample(list(range(len(train_Y))), k=self.batch_size)\n",
    "            batch_X = train_X[selected_index]\n",
    "            batch_Y = train_Y[selected_index]\n",
    "            self.session.run(self.train, {self.X_holder: batch_X, self.Y_holder: batch_Y})\n",
    "            step = i + 1\n",
    "            if step % self.print_per_batch == 0 or step == 1:\n",
    "                selected_index = random.sample(list(range(len(test_Y))), k=200)\n",
    "                batch_X = test_X[selected_index]\n",
    "                batch_Y = test_Y[selected_index]\n",
    "                loss_value, accuracy_value = self.session.run([self.loss, self.accuracy],\\\n",
    "                    {self.X_holder: batch_X, self.Y_holder: batch_Y})\n",
    "                used_time = time.time() - startTime\n",
    "                print('step:%d loss:%.4f accuracy:%.4f used time:%.2f seconds' %\n",
    "                      (step, loss_value, accuracy_value, used_time))\n",
    "\n",
    "    def predict(self, content_list):\n",
    "        if type(content_list) == str:\n",
    "            content_list = [content_list]\n",
    "        batch_X = self.content2X(content_list)\n",
    "        predict_y = self.session.run(self.predict_y, {self.X_holder:batch_X})\n",
    "        predict_label_list = self.labelEncoder.inverse_transform(predict_y)\n",
    "        return predict_label_list\n",
    "\n",
    "    def predictAll(self):\n",
    "        predict_label_list = []\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(self.test_content_list), batch_size):\n",
    "            content_list = self.test_content_list[i: i + batch_size]\n",
    "            predict_label = self.predict(content_list)\n",
    "            predict_label_list.extend(predict_label)\n",
    "        return predict_label_list\n",
    "\n",
    "    def printConfusionMatrix(self):\n",
    "        predict_label_list = self.predictAll()\n",
    "        df = pd.DataFrame(confusion_matrix(self.test_label_list, predict_label_list),\n",
    "                     columns=self.labelEncoder.classes_,\n",
    "                     index=self.labelEncoder.classes_)\n",
    "        print('\\n Confusion Matrix:')\n",
    "        print(df)\n",
    "\n",
    "    def printReportTable(self):\n",
    "        predict_label_list = self.predictAll()\n",
    "        reportTable = eval_model(self.test_label_list,\n",
    "                                 predict_label_list,\n",
    "                                 self.labelEncoder.classes_)\n",
    "        print('\\n Report Table:')\n",
    "        print(reportTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(y_true, y_pred, labels):\n",
    "    # 计算每个分类的Precision, Recall, f1, support\n",
    "    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred)\n",
    "    # 计算总体的平均Precision, Recall, f1, support\n",
    "    tot_p = np.average(p, weights=s)\n",
    "    tot_r = np.average(r, weights=s)\n",
    "    tot_f1 = np.average(f1, weights=s)\n",
    "    tot_s = np.sum(s)\n",
    "    res1 = pd.DataFrame({\n",
    "        u'Label': labels,\n",
    "        u'Precision': p,\n",
    "        u'Recall': r,\n",
    "        u'F1': f1,\n",
    "        u'Support': s\n",
    "    })\n",
    "    res2 = pd.DataFrame({\n",
    "        u'Label': ['总体'],\n",
    "        u'Precision': [tot_p],\n",
    "        u'Recall': [tot_r],\n",
    "        u'F1': [tot_f1],\n",
    "        u'Support': [tot_s]\n",
    "    })\n",
    "    res2.index = [999]\n",
    "    res = pd.concat([res1, res2])\n",
    "    return res[['Label', 'Precision', 'Recall', 'F1', 'Support']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('mailContent_list.pickle', 'rb') as file:\n",
    "    content_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('mailLabel_list.pickle', 'rb') as file:\n",
    "    label_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:1 loss:0.6732 accuracy:0.7000 used time:0.11 seconds\n",
      "step:250 loss:0.0440 accuracy:0.9800 used time:2.91 seconds\n",
      "step:500 loss:0.0289 accuracy:0.9850 used time:5.68 seconds\n",
      "step:750 loss:0.0507 accuracy:0.9850 used time:8.44 seconds\n",
      "step:1000 loss:0.0249 accuracy:0.9900 used time:11.23 seconds\n",
      "step:1250 loss:0.0398 accuracy:0.9900 used time:14.00 seconds\n",
      "step:1500 loss:0.0393 accuracy:0.9850 used time:16.78 seconds\n",
      "step:1750 loss:0.0024 accuracy:1.0000 used time:19.56 seconds\n",
      "step:2000 loss:0.0455 accuracy:0.9900 used time:22.36 seconds\n",
      "step:2250 loss:0.0052 accuracy:1.0000 used time:25.13 seconds\n",
      "step:2500 loss:0.0036 accuracy:1.0000 used time:27.90 seconds\n",
      "step:2750 loss:0.0204 accuracy:0.9950 used time:30.67 seconds\n",
      "step:3000 loss:0.0283 accuracy:0.9900 used time:33.49 seconds\n",
      "step:3250 loss:0.0030 accuracy:1.0000 used time:36.30 seconds\n",
      "step:3500 loss:0.0015 accuracy:1.0000 used time:39.09 seconds\n",
      "step:3750 loss:0.0224 accuracy:0.9950 used time:41.87 seconds\n",
      "step:4000 loss:0.0200 accuracy:0.9950 used time:44.65 seconds\n",
      "step:4250 loss:0.0011 accuracy:1.0000 used time:47.45 seconds\n",
      "step:4500 loss:0.0036 accuracy:1.0000 used time:50.24 seconds\n",
      "step:4750 loss:0.0190 accuracy:0.9900 used time:53.02 seconds\n",
      "step:5000 loss:0.0090 accuracy:0.9950 used time:55.80 seconds\n",
      "\n",
      " Confusion Matrix:\n",
      "       ham   spam\n",
      "ham   5409     61\n",
      "spam    13  10672\n",
      "\n",
      " Report Table:\n",
      "    Label  Precision    Recall        F1  Support\n",
      "0     ham   0.997231  0.987751  0.992469     5470\n",
      "1    spam   0.993760  0.998596  0.996172    10685\n",
      "999    总体   0.994935  0.994924  0.994918    16155\n"
     ]
    }
   ],
   "source": [
    "model = TextClassification(content_list, label_list)\n",
    "model.trainModel()\n",
    "model.printConfusionMatrix()\n",
    "model.printReportTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
